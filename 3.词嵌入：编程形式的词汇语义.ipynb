{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bdf37e8d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "word_to_ix={'hello':0,\"world\":1}\n",
    "embeds=nn.Embedding(2,5)\n",
    "lookup_tensor=torch.tensor([word_to_ix['hello']],dtype=torch.long)\n",
    "hello_embed=embeds(lookup_tensor)\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-61a7231e52c0>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-61a7231e52c0>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])foriinrange(len(test_sentence) - 2)]\u001b[0m\n\u001b[1;37m                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE=2\n",
    "EMBEDDING_DIM=10\n",
    "test_sentence=\"\"\"When forty winters shall besiege thy brow,Anddigdeeptrenchesinthybeauty's field,\n",
    "Thyyouth's proud livery so gazed on now,Willbeatotter'd weed of small worth held:Thenbeingasked, \n",
    "whereallthybeautylies,Whereallthetreasureofthylustydays;Tosay, withinthineowndeepsunkeneyes,Wereanall-eatingshame, \n",
    "andthriftlesspraise.Howmuchmorepraisedeserv'd thy beauty'suse,Ifthoucouldstanswer'\n",
    "This fair child of mineShallsummycount, andmakemyoldexcuse,'Provinghisbeautybysuccessionthine!Thisweretobene\n",
    "wmadewhenthouartold,Andseethybloodwarmwhenthoufeel'st it cold.\"\"\".split()\n",
    "# 应该对输入变量进行标记，但暂时忽略。\n",
    "# 创建一系列的元组，每个元组都是([ word_i-2, word_i-1 ], targetword)的形式。\n",
    "trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])foriinrange(len(test_sentence) - 2)]\n",
    "# 输出前3行，先看下是什么样子。\n",
    "print(trigrams[:3])vocab = set(test_sentence)word_to_ix = {word: ifori, wordinenumerate(vocab)}classNGramLanguageModeler(nn.Module):\n",
    "    def__init__(self, vocab_size, embedding_dim, context_size):super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)defforward(self, inputs):embeds = self.embeddings(inputs).view((1, -1))\n",
    "            out = F.relu(self.linear1(embeds))out = self.linear2(out)log_probs = F.log_softmax(out, dim=1)\n",
    "            returnlog_probslosses = []loss_function = nn.NLLLoss()model = NGramLanguageModeler(len(vocab), \n",
    "                                                                                               EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "            optimizer = optim.SGD(model.parameters(), lr=0.001)forepochinrange(10):total_loss = 0forcontext,\n",
    "                targetintrigrams:      \n",
    "                    # 步骤1\\. 准备好进入模型的数据(例如将单词转换成整数索引,并将其封装在变量中)\n",
    "                    context_idxs = torch.tensor([word_to_ix[w] forwincontext], dtype=torch.long)       \n",
    "                    # 步骤2\\. 回调torch累乘梯度        # 在传入一个新实例之前，需要把旧实例的梯度置零。\n",
    "                    model.zero_grad()        \n",
    "                    # 步骤3\\. 继续运行代码，得到单词的log概率值。\n",
    "                    log_probs = model(context_idxs)        \n",
    "# 步骤4\\. 计算损失函数（再次注意，Torch需要将目标单词封装在变量里）。\n",
    "loss = loss_function(log_probs, torch.tensor([word_to_ix[target]],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-725d406715f3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-725d406715f3>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    dtype=torch.long))\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dtype=torch.long))       \n",
    "# 步骤5\\. 反向传播更新梯度loss.backward()optimizer.step()        \n",
    "# 通过调tensor.item()得到单个Python数值。\n",
    "total_loss+=loss.item()losses.append(total_loss)print(losses)  # 用训练数据每次迭代，损失函数都会下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习：计算连续词袋模型的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTEXT_SIZE=2  # 左右各两个词raw_text=\"\"\"We are about to study the idea of a computational process.Computationalprocessesareabstractbeingsthatinhabitcomputers.Astheyevolve, processesmanipulateotherabstractthingscalleddata.Theevolutionofaprocessisdirectedbyapatternofrulescalledaprogram. Peoplecreateprogramstodirectprocesses. Ineffect,weconjurethespiritsofthecomputerwithourspells.\"\"\".split()# 通过对`raw_text`使用set()函数，我们进行去重操作vocab=set(raw_text)vocab_size=len(vocab)word_to_ix= {word: ifori, wordinenumerate(vocab)}data= []foriinrange(2, len(raw_text)-2):context= [raw_text[i-2], raw_text[i-1],raw_text[i+1], raw_text[i+2]]target=raw_text[i]data.append((context, target))print(data[:5])classCBOW(nn.Module):def__init__(self):passdefforward(self, inputs):pass# 创建模型并且训练。这里有些函数帮你在使用模块之前制作数据。defmake_context_vector(context, word_to_ix):idxs= [word_to_ix[w] forwincontext]returntorch.tensor(idxs, dtype=torch.long)make_context_vector(data[0][0], word_to_ix)  # exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
